<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cs422 on Jared Rosario</title>
    <link>https://kaonpositive.github.io/tags/cs422/</link>
    <description>Recent content in cs422 on Jared Rosario</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 10 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://kaonpositive.github.io/tags/cs422/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CS 422 Notes 09</title>
      <link>https://kaonpositive.github.io/posts/cs422notes09/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes09/</guid>
      <description>Entropy It is the measure of uncertainty in a probability distribution.
Formula: $\mathbb{H}(x) = -\sum_k^kP(X=k)*logp(X=k)$
Gross Entropy Gross Entropy of two distributions P and Q is given by: $\mathbb{H}(p,q) = -\sum_{k=1}^kP_k*log(q_k)$
 Gross Entropy is used as log loss in binary classification.
 Generate Classifier $P(y = c|x, \theta) \alpha P(x|y = c, \theta)P(y=c | \theta)$
 NOTE: Alpha means proportional
 Difference between Discrimitive and Generative Classifiers  Discrimitive Classifier finds a boundry that separates the two classes.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 07</title>
      <link>https://kaonpositive.github.io/posts/cs422notes07/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes07/</guid>
      <description>Statistics The task of estimating the parameters ($\theta$) from the training data is called training\model fitting.
$\hat{\theta} = argmin L ({\theta})$
 $\hat{\theta}$ = estimated parameter $L = $ Loss Function (aka objective/cost function)  argmin $f(\theta)$ returns $\theta$ that minimizes $L(\theta)$. Ex: argmin $x^2$ = 0
Maximum Likelihood Estimate (MLE) Refers to finding parameters that maximizes one likelihood of training data. It is the most common approach to parameter estimation.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 08</title>
      <link>https://kaonpositive.github.io/posts/cs422notes08/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes08/</guid>
      <description>Bayesian Decision Theory Allows to choose the best action based on the given situation.
 Baye&amp;rsquo;s Rule will show up on the exam. An example of Baye&amp;rsquo;s rule is on Section 2.3.1 of the textbook (COVID-19).
 </description>
    </item>
    
    <item>
      <title>CS 422 Notes 06</title>
      <link>https://kaonpositive.github.io/posts/cs422notes06/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes06/</guid>
      <description>Bernoulli Distribution Used to represent the distribution of a binary outcome. For example, tossing a coin.
 y = 1 &amp;lt;- &amp;lsquo;heads&amp;rsquo; $P(y=1) = \theta$ y = 0 &amp;lt;- &amp;lsquo;tails&amp;rsquo; $P(y=0) = 1 - \theta$  PMF is defined as: $P(y | \theta) = \theta$ if $y = 1$ $P(y | \theta) = 1 - \theta$ if $y = 0$
Binomial Distributions Used to represent the distribution of a repeated binary outcome.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 05</title>
      <link>https://kaonpositive.github.io/posts/cs422notes05/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes05/</guid>
      <description>Random Variables It is a variable that has unknown/random quantity of interest.
${\mathcal X}$: the set of all possible values. Also called a sample space or static space.
Discrete Random Variables Sample space is finite/countably infinite (${\mathcal X}$). Probability of event X = x is denoted by $P(X=x)$ or $P(x)$ for short.
 x: Random Variable (ex: heads/tails) ${\mathcal X}$: Sample Space (ex: S={Heads, Tails}) x (small x) -&amp;gt; x: one of the possible values (ex: x=heads, or x = tails)  Continuous Random Variables If x (a random variable) ${\in\mathbb{R}}$ is a real-valued quantity, it is called a continous random variable.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 04</title>
      <link>https://kaonpositive.github.io/posts/cs422notes04/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes04/</guid>
      <description>Probability Probability quantifies uncertainty.
 Probability is between 0 and 1.
 P(A): Probability that the event A will occur.
 0 &amp;lt;= P(A) &amp;lt;= 1  P(${A&#39;}$): Probability that the event A will NOT occur.
 $0 &amp;lt;= P({A^c}) &amp;lt;= 1$ $P(A) + P({A^c}) = 1$ $P({A^c}) = 1 - P(A)$  Joint Probability Define probability of two events occuring at the same time. ${P(A \cap B}) = P(A, B)$</description>
    </item>
    
    <item>
      <title>CS 422 Notes 03</title>
      <link>https://kaonpositive.github.io/posts/cs422notes03/</link>
      <pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes03/</guid>
      <description>What is Unsupervised Learning? It is a learning model where it is trained ONLY on the inputs.
 REMEMBER: Supervised learning uses input-output pairs to parameterize the model for improvement.
  Ex of Unsupervised Learning: Music recommendations by using clusters of users that have similar music taste.  What is Clustering? No classification exists to determine data clusters. Therefore, it is a challenge to separate data in their respective classes/groups.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 02</title>
      <link>https://kaonpositive.github.io/posts/cs422notes02/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes02/</guid>
      <description>$ {y_n} = f({x_n})$ where $f$ is the fuction we assume exist and try to approximate.
$ \hat{y_n} = f^({x_n})$ where $ \hat{y_n} $ is the prediction and $ f^ $ is the model&amp;rsquo;s approximation.
If we try to get comparisons for a data instance, we try to find the best line that fits the data instance the best.
 Ex: you will prefer a function over another function since data is closests to it.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 01</title>
      <link>https://kaonpositive.github.io/posts/cs422notes01/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes01/</guid>
      <description>Supervised Learning continued&amp;hellip;  x $ {\in} $ X (capital X is the input space) y $ {\in} $ Y (capital Y is the output space)   Basically, the input space is the set of possible inputs; output space is the set of possible outputs.
  D = {(${x_n}, {y_n}$)}${_n}^{N}$ is the set of all input-output pairs.  N is the dataset size and D is script D.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 00</title>
      <link>https://kaonpositive.github.io/posts/cs422notes00/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes00/</guid>
      <description>What is Machine Learning? By definition from Tom Mitchell:
 A computer program is learning form experience E w/ respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by P, improves w/ experience E.
 Breaking it down into simple terms&amp;hellip;
 You improve over tasks T.   Ex: Predicting the price of the house in the market.  W/ repect to some performance P   Ex: How much the house is sold for.</description>
    </item>
    
  </channel>
</rss>
