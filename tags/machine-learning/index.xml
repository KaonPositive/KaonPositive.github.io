<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Jared Rosario</title>
    <link>https://kaonpositive.github.io/tags/machine-learning/</link>
    <description>Recent content in machine learning on Jared Rosario</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kaonpositive.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ML Notes1</title>
      <link>https://kaonpositive.github.io/posts/notesresearch1/</link>
      <pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/notesresearch1/</guid>
      <description>CUDA Programming on Jetson Nano Purpose of CUDA Compute Unified Device Architecture (CUDA) is used for parallel computing on CUDA-enabled GPUs.
Architecture CPU is meant for sequential execution of instructions GPU is meant for parallel execution of control logic.
By using CUDA&amp;rsquo;s parallel processing feature, it breaks down tasks into thousands of smaller threads that can be executed independently&amp;ndash;this would lead to higher efficiency and parallelism.
TLDR: Basically, CUDA allows parallel processing of tasks to speed their applications by using GPU accelerators.</description>
    </item>
    
    <item>
      <title>ML Notes0</title>
      <link>https://kaonpositive.github.io/posts/notesresearch0/</link>
      <pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/notesresearch0/</guid>
      <description>Setup Purpose of making these notes is for the YOLOv8 with Roboflow CV pipeline on Google Colab.
Step 1 - Install Dependencies Installing dependencies is the first step for this. Two important dependencies to install:
 ultralytics roboflow  Code to install dependencies:
!pip install ultralytics !pip install roboflow  NOTE: Before installing dependencies, make sure to have the runtime to be set using the GPU and not the CPU. Using the GPU will allow the model to utilize parallelism&amp;ndash;this enables CUDA.</description>
    </item>
    
    <item>
      <title>Machine Learning Notes</title>
      <link>https://kaonpositive.github.io/posts/generalml/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/generalml/</guid>
      <description>Goal for Machine Learning The goal for machine learning is to help us make decisions with or without human supervision. To achieve this, machine learning uses a group of algorithms/methodologies to discover and formulate repeatable patterns in data.
Unsupervised Learning An algorithm that detects and utilizes data that are not generated randomly, therefore it finds patterns/structure within the data to comprehend it.
 Laymen&amp;rsquo;s Terms: Non-randomly generated data is read and the algorithm will detect certain patterns within the data to better understand it.</description>
    </item>
    
    <item>
      <title>CS422 Final Study Guide</title>
      <link>https://kaonpositive.github.io/posts/cs422finalreview/</link>
      <pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422finalreview/</guid>
      <description>Logistic Regression Is a classification algorithm.
Is a discriminative classification algorithm that has the form $P(y|x, \theta)$ where $X\in R^D$ (basically, it&amp;rsquo;s continuous and D-dimensional).
 NOTE: Logistic Regression can use categorical inputs. Naive Bayes is a generative algorithm.
 If output size is:
 C = 2, then it is classified as binary logistic regression. C &amp;gt; 2, then it is classified as multi-class logistic regression.  Binary Logistic Regression $P(y|x, \theta)$ = Ber$( y|\mu(x))$</description>
    </item>
    
    <item>
      <title>CS 422 Notes 09</title>
      <link>https://kaonpositive.github.io/posts/cs422notes09/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes09/</guid>
      <description>Entropy It is the measure of uncertainty in a probability distribution.
Formula: $\mathbb{H}(x) = -\sum_k^kP(X=k)*logp(X=k)$
Gross Entropy Gross Entropy of two distributions P and Q is given by: $\mathbb{H}(p,q) = -\sum_{k=1}^kP_k*log(q_k)$
 Gross Entropy is used as log loss in binary classification.
 Generate Classifier $P(y = c|x, \theta) \alpha P(x|y = c, \theta)P(y=c | \theta)$
 NOTE: Alpha means proportional
 Difference between Discrimitive and Generative Classifiers  Discrimitive Classifier finds a boundry that separates the two classes.</description>
    </item>
    
    <item>
      <title>CS 422 Midterm Review</title>
      <link>https://kaonpositive.github.io/posts/cs422midtermreview/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422midtermreview/</guid>
      <description>What is Supervised Learning? Function that perfectly maps each input to its output.
 Supervised Learning Algorithms work with input-output pairs!
 f* &amp;lt;- model&amp;rsquo;s approximation x (bold lowercase) = input vector y = output (typically scalar unless output is multi-dimensional)
What is a model parameter? Variable which can be estimatted by fitting given data to model.
 ex: $f(x) = mx+c$, where x = independent variable, c = dependent   m and c are parameters.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 07</title>
      <link>https://kaonpositive.github.io/posts/cs422notes07/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes07/</guid>
      <description>Statistics The task of estimating the parameters ($\theta$) from the training data is called training\model fitting.
$\hat{\theta} = argmin L ({\theta})$
 $\hat{\theta}$ = estimated parameter $L = $ Loss Function (aka objective/cost function)  argmin $f(\theta)$ returns $\theta$ that minimizes $L(\theta)$. Ex: argmin $x^2$ = 0
Maximum Likelihood Estimate (MLE) Refers to finding parameters that maximizes one likelihood of training data. It is the most common approach to parameter estimation.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 08</title>
      <link>https://kaonpositive.github.io/posts/cs422notes08/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes08/</guid>
      <description>Bayesian Decision Theory Allows to choose the best action based on the given situation.
 Baye&amp;rsquo;s Rule will show up on the exam. An example of Baye&amp;rsquo;s rule is on Section 2.3.1 of the textbook (COVID-19).
 </description>
    </item>
    
    <item>
      <title>CS 422 Notes 06</title>
      <link>https://kaonpositive.github.io/posts/cs422notes06/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes06/</guid>
      <description>Bernoulli Distribution Used to represent the distribution of a binary outcome. For example, tossing a coin.
 y = 1 &amp;lt;- &amp;lsquo;heads&amp;rsquo; $P(y=1) = \theta$ y = 0 &amp;lt;- &amp;lsquo;tails&amp;rsquo; $P(y=0) = 1 - \theta$  PMF is defined as: $P(y | \theta) = \theta$ if $y = 1$ $P(y | \theta) = 1 - \theta$ if $y = 0$
Binomial Distributions Used to represent the distribution of a repeated binary outcome.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 05</title>
      <link>https://kaonpositive.github.io/posts/cs422notes05/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes05/</guid>
      <description>Random Variables It is a variable that has unknown/random quantity of interest.
${\mathcal X}$: the set of all possible values. Also called a sample space or static space.
Discrete Random Variables Sample space is finite/countably infinite (${\mathcal X}$). Probability of event X = x is denoted by $P(X=x)$ or $P(x)$ for short.
 x: Random Variable (ex: heads/tails) ${\mathcal X}$: Sample Space (ex: S={Heads, Tails}) x (small x) -&amp;gt; x: one of the possible values (ex: x=heads, or x = tails)  Continuous Random Variables If x (a random variable) ${\in\mathbb{R}}$ is a real-valued quantity, it is called a continous random variable.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 04</title>
      <link>https://kaonpositive.github.io/posts/cs422notes04/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes04/</guid>
      <description>Probability Probability quantifies uncertainty.
 Probability is between 0 and 1.
 P(A): Probability that the event A will occur.
 0 &amp;lt;= P(A) &amp;lt;= 1  P(${A&#39;}$): Probability that the event A will NOT occur.
 $0 &amp;lt;= P({A^c}) &amp;lt;= 1$ $P(A) + P({A^c}) = 1$ $P({A^c}) = 1 - P(A)$  Joint Probability Define probability of two events occuring at the same time. ${P(A \cap B}) = P(A, B)$</description>
    </item>
    
    <item>
      <title>CS 422 Notes 03</title>
      <link>https://kaonpositive.github.io/posts/cs422notes03/</link>
      <pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes03/</guid>
      <description>What is Unsupervised Learning? It is a learning model where it is trained ONLY on the inputs.
 REMEMBER: Supervised learning uses input-output pairs to parameterize the model for improvement.
  Ex of Unsupervised Learning: Music recommendations by using clusters of users that have similar music taste.  What is Clustering? No classification exists to determine data clusters. Therefore, it is a challenge to separate data in their respective classes/groups.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 02</title>
      <link>https://kaonpositive.github.io/posts/cs422notes02/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes02/</guid>
      <description>$ {y_n} = f({x_n})$ where $f$ is the fuction we assume exist and try to approximate.
$ \hat{y_n} = f^({x_n})$ where $ \hat{y_n} $ is the prediction and $ f^ $ is the model&amp;rsquo;s approximation.
If we try to get comparisons for a data instance, we try to find the best line that fits the data instance the best.
 Ex: you will prefer a function over another function since data is closests to it.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 01</title>
      <link>https://kaonpositive.github.io/posts/cs422notes01/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes01/</guid>
      <description>Supervised Learning continued&amp;hellip;  x $ {\in} $ X (capital X is the input space) y $ {\in} $ Y (capital Y is the output space)   Basically, the input space is the set of possible inputs; output space is the set of possible outputs.
  D = {(${x_n}, {y_n}$)}${_n}^{N}$ is the set of all input-output pairs.  N is the dataset size and D is script D.</description>
    </item>
    
    <item>
      <title>CS 422 Notes 00</title>
      <link>https://kaonpositive.github.io/posts/cs422notes00/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://kaonpositive.github.io/posts/cs422notes00/</guid>
      <description>What is Machine Learning? By definition from Tom Mitchell:
 A computer program is learning form experience E w/ respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by P, improves w/ experience E.
 Breaking it down into simple terms&amp;hellip;
 You improve over tasks T.   Ex: Predicting the price of the house in the market.  W/ repect to some performance P   Ex: How much the house is sold for.</description>
    </item>
    
  </channel>
</rss>
